{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6naHbwdvzTkl",
        "outputId": "6c26aa9f-dfe5-41eb-fb10-c6309fc2130f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.86)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax) (4.12.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "QMMcqS9TygQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKBwF6wry8xJ",
        "outputId": "4846aa44-91a6-4bcf-f7e6-ed6f039701af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-25 00:17:56--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-08-25 00:17:56 (114 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "N3KWHGhUyjqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and preprocessing\n",
        "def load_data():\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "    data = jnp.array(encode(text), dtype=jnp.int32)\n",
        "    n = int(0.9 * len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]\n",
        "    return train_data, val_data, encode, decode, vocab_size\n"
      ],
      "metadata": {
        "id": "fXK8hbwh0E0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, encode, decode, vocab_size = load_data()"
      ],
      "metadata": {
        "id": "CeVmMyT60IjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, key):\n",
        "    data_size = data.shape[0]\n",
        "    key, subkey = jax.random.split(key)\n",
        "    ix = jax.random.randint(subkey, (batch_size,), 0, data_size - block_size)\n",
        "    x = jnp.take(data, jnp.arange(block_size)[None, :] + ix[:, None], axis=0)\n",
        "    y = jnp.take(data, jnp.arange(1, block_size + 1)[None, :] + ix[:, None], axis=0)\n",
        "    return x, y, key"
      ],
      "metadata": {
        "id": "a9CL3mPG0Gab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg5k7nvDyURN"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        B, T, C = x.shape\n",
        "        key = nn.Dense(features=self.head_size, use_bias=False)(x)\n",
        "        query = nn.Dense(features=self.head_size, use_bias=False)(x)\n",
        "        value = nn.Dense(features=self.head_size, use_bias=False)(x)\n",
        "\n",
        "        tril = jnp.tril(jnp.ones((T, T)))\n",
        "\n",
        "        wei = (query @ key.transpose(0, 2, 1)) * (self.head_size ** -0.5)\n",
        "        wei = jnp.where(tril == 0, float('-inf'), wei)\n",
        "        wei = jax.nn.softmax(wei, axis=-1)\n",
        "        wei = nn.Dropout(rate=dropout)(wei, deterministic=not training)\n",
        "\n",
        "        out = wei @ value\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        heads = [Head(self.head_size) for _ in range(self.num_heads)]\n",
        "        out = jnp.concatenate([head(x, training) for head in heads], axis=-1)\n",
        "        out = nn.Dense(features=n_embd)(out)\n",
        "        out = nn.Dropout(rate=dropout)(out, deterministic=not training)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        x = nn.Dense(features=4 * n_embd)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(features=n_embd)(x)\n",
        "        x = nn.Dropout(rate=dropout)(x, deterministic=not training)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        x = x + MultiHeadAttention(num_heads=n_head, head_size=n_embd // n_head)(nn.LayerNorm()(x), training)\n",
        "        x = x + FeedForward()(nn.LayerNorm()(x), training)\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    vocab_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx, targets=None, training: bool = False):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = nn.Embed(num_embeddings=self.vocab_size, features=n_embd)(idx)\n",
        "        pos_emb = nn.Embed(num_embeddings=block_size, features=n_embd)(jnp.arange(T))\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        for _ in range(n_layer):\n",
        "            x = Block()(x, training)\n",
        "\n",
        "        x = nn.LayerNorm()(x)\n",
        "        logits = nn.Dense(features=self.vocab_size)(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.reshape(B*T, C)\n",
        "            targets = targets.reshape(B*T)\n",
        "            # loss = jax.nn.sparse_categorical_crossentropy(logits, targets, from_logits=True).mean()\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, params, idx, max_new_tokens, key):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self.apply(params, idx_cond, training=False, rngs={'dropout': key})\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = jax.nn.softmax(logits, axis=-1)\n",
        "            idx_next = jax.random.categorical(key, probs, axis=-1)\n",
        "            idx = jnp.concatenate([idx, idx_next[:, None]], axis=1)\n",
        "            key, _ = jax.random.split(key)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "vocab_size = 65  # Example value, adjust as needed\n",
        "model = GPTLanguageModel(vocab_size=vocab_size)\n",
        "key = jax.random.PRNGKey(0)\n",
        "params = model.init(key, jnp.ones((1, block_size), dtype=jnp.int32))\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Training loop\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, x, y):\n",
        "    def loss_fn(params):\n",
        "        _, loss = model.apply(params, x, y, training=True)\n",
        "        return loss\n",
        "\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "@jax.jit\n",
        "def loss_fn(params, x, y):\n",
        "    _, loss = model.apply(params, x, y, training=True)\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "    _, loss = model.apply(params, x, y, training=False)\n",
        "    return loss\n",
        "\n",
        "def estimate_loss(params, key):\n",
        "    losses = {split: [] for split in ['train', 'val']}\n",
        "    for split in ['train', 'val']:\n",
        "        data = train_data if split == 'train' else val_data\n",
        "        for _ in range(eval_iters):\n",
        "            key, subkey = jax.random.split(key)\n",
        "            x, y, key = get_batch(data, subkey)\n",
        "            loss = eval_step(params, x, y)\n",
        "            losses[split].append(loss)\n",
        "    return {k: jnp.mean(jnp.array(v)) for k, v in losses.items()}, key\n",
        "\n",
        "# Main training loop\n",
        "key = jax.random.PRNGKey(1)  # New key for training\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses, key = estimate_loss(params, key)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Sample a batch of data\n",
        "    key, subkey = jax.random.split(key)\n",
        "    x, y, key = get_batch(train_data, subkey)\n",
        "    loss = loss_fn(params, x, y)\n",
        "    grads = jax.grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n"
      ],
      "metadata": {
        "id": "lxCtiMf704_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation\n",
        "gen_key = jax.random.PRNGKey(2)\n",
        "context = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "generated = model.generate(params, context, max_new_tokens=500, key=gen_key)\n",
        "print(decode(generated[0].tolist()))"
      ],
      "metadata": {
        "id": "nnyxqLEa1jTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TsStkxy00mK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}