# ğŸ§  Simple GPT-2 Implementation in JAX/Flax ğŸš€

This project implements a simplified version of the GPT-2 language model using JAX and [Flax](https://flax.readthedocs.io/en/latest/). Its based on the PyTorch GPT2 implementation by Andrej Karpathy - https://github.com/karpathy/ng-video-lecture

## ğŸŒŸ Features

- ğŸ“š Trains on the TinyShakespeare dataset
- ğŸ§® Implements multi-head attention and transformer blocks
- ğŸ”§ Customizable hyperparameters
- ğŸ“ˆ Training and validation loss tracking
- ğŸ­ Text generation capability

## ğŸ› ï¸ Requirements

You can install the requirements using the provided `requirements.txt` file:

```bash
pip install -r requirements.txt
```

## ğŸš€ Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/suryavanshi/jax_llm.git
   cd jax_llm
   ```

2. Install the requirements:
   ```bash
   pip install -r requirements.txt
   ```

3. Start Jupyter lab:
   ```
   jupyter lab 
   ```

## ğŸ›ï¸ Hyperparameters

You can adjust the following hyperparameters in the script:

- `batch_size`: Number of sequences per batch
- `block_size`: Maximum context length for predictions
- `max_iters`: Number of training iterations
- `eval_interval`: How often to evaluate the model
- `learning_rate`: Learning rate for the Adam optimizer
- `n_embd`: Embedding dimension
- `n_head`: Number of attention heads
- `n_layer`: Number of transformer layers
- `dropout`: Dropout rate

## ğŸ“Š Model Architecture

The model consists of:
- Token and position embeddings
- Multiple transformer blocks with:
  - Multi-head self-attention
  - Feed-forward neural networks
- Layer normalization
- Final linear layer for next token prediction

## ğŸ­ Generating Text

After training, you can generate text using the `generate` method of the `GPTLanguageModel` class.

## ğŸ“œ License

This project is [MIT](https://choosealicense.com/licenses/mit/) licensed.

## ğŸ™ Acknowledgements

- [Andrej Karpathy's](https://github.com/karpathy) educational content on language models
- The JAX and Flax teams for their excellent libraries

