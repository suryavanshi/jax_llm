{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYt7oxIhJaOF"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "import optax\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj0YR9tdJiOq",
        "outputId": "0afd6e3a-2c17-4634-e0ef-9e63f2d794a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-25 01:31:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-08-25 01:31:19 (40.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_uG9ymWJk32"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 500\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 50\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FngLknz_JxE6"
      },
      "outputs": [],
      "source": [
        "# Data loading and preprocessing\n",
        "def load_data():\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "    data = jnp.array(encode(text), dtype=jnp.int32)\n",
        "    n = int(0.9 * len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]\n",
        "    return train_data, val_data, encode, decode, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCNt3Oz-JzmE"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, encode, decode, vocab_size = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjQmeaDxJ0OP"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, key):\n",
        "    data_size = data.shape[0]\n",
        "    key, subkey = jax.random.split(key)\n",
        "    ix = jax.random.randint(subkey, (batch_size,), 0, data_size - block_size)\n",
        "    x = jnp.take(data, jnp.arange(block_size)[None, :] + ix[:, None], axis=0)\n",
        "    y = jnp.take(data, jnp.arange(1, block_size + 1)[None, :] + ix[:, None], axis=0)\n",
        "    return x, y, key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev5FJO3WJc6S"
      },
      "outputs": [],
      "source": [
        "class Head(nnx.Module):\n",
        "    def __init__(self, head_size: int, rngs: nnx.Rngs):\n",
        "        self.key = nnx.Linear(n_embd, head_size, use_bias=False, rngs=rngs)\n",
        "        self.query = nnx.Linear(n_embd, head_size, use_bias=False, rngs=rngs)\n",
        "        self.value = nnx.Linear(n_embd, head_size, use_bias=False, rngs=rngs)\n",
        "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        att = (q @ k.transpose(0, 2, 1)) * (k.shape[-1] ** -0.5)\n",
        "        att = jnp.where(jnp.tril(jnp.ones((T, T))) == 0, float('-inf'), att)\n",
        "        att = jax.nn.softmax(att, axis=-1)\n",
        "        att = self.dropout(att, deterministic=not training)\n",
        "\n",
        "        return att @ v\n",
        "\n",
        "class MultiHeadAttention(nnx.Module):\n",
        "    def __init__(self, num_heads: int, head_size: int, rngs: nnx.Rngs):\n",
        "        self.heads = [Head(head_size, rngs=rngs) for _ in range(num_heads)]\n",
        "        self.proj = nnx.Linear(num_heads * head_size, n_embd, rngs=rngs)\n",
        "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        out = jnp.concatenate([h(x, training=training) for h in self.heads], axis=-1)\n",
        "        out = self.dropout(self.proj(out), deterministic=not training)\n",
        "        return out\n",
        "\n",
        "# class FeedForward(nnx.Module):\n",
        "#     def __init__(self, rngs: nnx.Rngs):\n",
        "#         self.net = nnx.Sequential([\n",
        "#             nnx.Linear(n_embd, 4 * n_embd, rngs=rngs),\n",
        "#             jax.nn.relu,\n",
        "#             nnx.Linear(4 * n_embd, n_embd, rngs=rngs),\n",
        "#             nnx.Dropout(dropout, rngs=rngs)\n",
        "#         ])\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "class FeedForward(nnx.Module):\n",
        "    def __init__(self, rngs: nnx.Rngs):\n",
        "        self.net = [\n",
        "            nnx.Linear(n_embd, 4 * n_embd, rngs=rngs),\n",
        "            nnx.Dropout(dropout, rngs=rngs),\n",
        "            nnx.Linear(4 * n_embd, n_embd, rngs=rngs),\n",
        "            nnx.Dropout(dropout, rngs=rngs)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nnx.Dropout):\n",
        "                x = layer(x, deterministic=not training)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            if layer == self.net[0]:  # After the first linear layer\n",
        "                x = jax.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "class Block(nnx.Module):\n",
        "    def __init__(self, rngs: nnx.Rngs):\n",
        "        self.sa = MultiHeadAttention(n_head, n_embd // n_head, rngs=rngs)\n",
        "        self.ffwd = FeedForward(rngs=rngs)\n",
        "        self.ln1 = nnx.LayerNorm(n_embd, rngs=rngs)\n",
        "        self.ln2 = nnx.LayerNorm(n_embd, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x, training: bool = False):\n",
        "        x = x + self.sa(self.ln1(x), training=training)\n",
        "        x = x + self.ffwd(self.ln2(x), training=training)\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nnx.Module):\n",
        "    def __init__(self, vocab_size: int, rngs: nnx.Rngs):\n",
        "        self.token_embedding_table = nnx.Embed(vocab_size, n_embd, rngs=rngs)\n",
        "        self.position_embedding_table = nnx.Embed(block_size, n_embd, rngs=rngs)\n",
        "        # self.blocks = nnx.Sequential([Block(rngs=rngs) for _ in range(n_layer)])\n",
        "        self.blocks = [Block(rngs=rngs) for _ in range(n_layer)]\n",
        "        self.ln_f = nnx.LayerNorm(n_embd, rngs=rngs)\n",
        "        self.lm_head = nnx.Linear(n_embd, vocab_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, idx, targets=None, training: bool = False):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(jnp.arange(T))\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # x = self.blocks(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training=training)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.reshape(B*T, C)\n",
        "            targets = targets.reshape(B*T)\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = jax.nn.softmax(logits, axis=-1)\n",
        "            idx_next = jax.random.categorical(jax.random.PRNGKey(0), probs, axis=-1)\n",
        "            idx = jnp.concatenate([idx, idx_next[:, None]], axis=1)\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh2-PctYJuK8",
        "outputId": "997a0be6-cc56-472d-cece-acb0d780f877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Step 0: train loss 4.7810, val loss 4.7506, time: 19.93s\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-2972ccb9369b>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "# Initialize model and optimizer\n",
        "model = GPTLanguageModel(vocab_size, rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(learning_rate))\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, x, y):\n",
        "    def loss_fn(model):\n",
        "        _, loss = model(x, y, training=True)\n",
        "        return loss\n",
        "\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "    optimizer.update(grads)\n",
        "\n",
        "    return loss\n",
        "\n",
        "@nnx.jit\n",
        "def eval_model(model, x, y):\n",
        "    _, loss = model(x, y, training=False)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "key = jax.random.PRNGKey(0)\n",
        "start_time = time.time()\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = {'train': 0.0, 'val': 0.0}\n",
        "        for split in ['train', 'val']:\n",
        "            data = train_data if split == 'train' else val_data\n",
        "            for _ in range(eval_iters):\n",
        "                key, subkey = jax.random.split(key)\n",
        "                x, y, key = get_batch(data, subkey)\n",
        "                loss = eval_model(model, x, y)\n",
        "                losses[split] += loss / eval_iters\n",
        "        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    x, y = get_batch(train_data, subkey)\n",
        "    loss = train_step(model, optimizer, x, y)\n",
        "\n",
        "    if iter % 10 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss:.4f}\", end='\\r', flush=True)\n",
        "\n",
        "print(\"\\nTraining completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znuUOzhxJhVx"
      },
      "outputs": [],
      "source": [
        "# Generate sample text\n",
        "print(\"\\nGenerating sample text...\")\n",
        "context = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "generated = model.generate(context, max_new_tokens=100)\n",
        "print(decode(generated[0].tolist()))\n",
        "\n",
        "# Helper functions (make sure these are defined)\n",
        "# def get_batch(data):\n",
        "#     ix = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, len(data) - block_size)\n",
        "#     x = jnp.stack([data[i:i+block_size] for i in ix])\n",
        "#     y = jnp.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "#     return x, y\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}